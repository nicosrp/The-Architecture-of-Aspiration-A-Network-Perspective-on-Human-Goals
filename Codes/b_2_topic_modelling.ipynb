{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8603df50-b6aa-4197-843c-26b058559abb",
   "metadata": {},
   "source": [
    "# B.2 - Topic Classification - BERT Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0c385c-6769-4238-b7be-ffb72318659a",
   "metadata": {},
   "source": [
    "In this notebook, we utilize S-BERT embeddings with k-means clustering to discover topics within our goals. To do so, we import our network, create embeddings and assign each node one topic. We then extract relevant topic keywords for each topic. Finally, we export the network with the created topics to start a manual open-coding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef841990-5e7b-4258-b013-0703c5222145",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Besitzer\\Desktop\\M.Sc. Social Data Science\\3. Semester\\Social graphs and interactions\\DayZero\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# importing all relevant packages\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import pickle\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a79f0759-50c0-4d4f-b384-f5a4bd87ead2",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/nicosrp/The-Architecture-of-Aspiration-A-Network-Perspective-on-Human-Goals/main/Networks/Prior%20Network%20Versions/b1_network.pkl\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "G = pickle.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f1b5912-1ab8-4bd4-a079-b5fb973254ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'wants_to_do', 'have_done', 'merged_goals', 'included_by_our_users', 'description', 'tags', 'comments', 'title'}\n"
     ]
    }
   ],
   "source": [
    "# current node attributes\n",
    "attr_keys = {k for _, attrs in G.nodes(data=True) for k in attrs}\n",
    "print(attr_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea494fd7-0855-4aa5-a9ea-7145c62f580d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2890"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking the number of nodes to ensure network is correct\n",
    "len(G.nodes())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f83687-1734-434c-ad90-5dd431b8ddd0",
   "metadata": {},
   "source": [
    "## S-BERT with K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b2f0b5-b628-49b8-b52a-cd09f7cdde55",
   "metadata": {},
   "source": [
    "First, we extract titles and descriptions of our nodes and combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f0faf43-af5e-464a-9822-5705daaf0590",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "node_ids = []\n",
    "\n",
    "for node, attrs in G.nodes(data=True):\n",
    "    title = attrs.get(\"title\", \"\")\n",
    "    description = attrs.get(\"description\", \"\")\n",
    "    \n",
    "    # Combined text for topic modeling\n",
    "    text = f\"{title}. {description}\".strip()\n",
    "    \n",
    "    texts.append(text)\n",
    "    node_ids.append(node)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca037878-3a61-4bd8-a9aa-13966d7669be",
   "metadata": {},
   "source": [
    "Next, we set up our model and create embeddings using the texts extracted from the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c5e1c7d-63ce-49cf-b238-a4ec4b178f10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 91/91 [00:32<00:00,  2.78it/s]\n"
     ]
    }
   ],
   "source": [
    "# Use a reasonably small but strong model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=\"cpu\")\n",
    "\n",
    "# Create embeddings (matrix of size 2900 x 384)\n",
    "embeddings = model.encode(\n",
    "    texts,\n",
    "    batch_size=32,\n",
    "    show_progress_bar=True,\n",
    "    convert_to_numpy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "632dcb05-77f7-44ce-94b0-abaaf5ff47f0",
   "metadata": {},
   "source": [
    "Then we use k-means clustering with 20 clusters on the created embeddings. We choose a random state for replicability. The amount of clusters was chosen on a basis of trial and error, resulting in coherent yet not too specific topics. Finally, we assign the topic labels back to the nodes in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65841911-5909-47e0-8b45-999668a4bafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of clusters\n",
    "num_clusters = 20\n",
    "\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(embeddings)\n",
    "\n",
    "# Assign topic labels back to graph nodes\n",
    "for node, topic in zip(node_ids, cluster_labels):\n",
    "    G.nodes[node][\"topic\"] = int(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f2de60-19e7-471d-8c95-3be17393c727",
   "metadata": {},
   "source": [
    "To get an overview over the distribution of the topics created above and the assigned nodes, we print the counts of the nodes assigned to each topic/cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb5694fa-30f3-4a3f-8070-54c3d6d2c46a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 156\n",
      "1 193\n",
      "2 164\n",
      "3 83\n",
      "4 179\n",
      "5 181\n",
      "6 210\n",
      "7 220\n",
      "8 117\n",
      "9 172\n",
      "10 191\n",
      "11 136\n",
      "12 75\n",
      "13 80\n",
      "14 172\n",
      "15 180\n",
      "16 103\n",
      "17 81\n",
      "18 98\n",
      "19 99\n"
     ]
    }
   ],
   "source": [
    "# Extract all topic values from nodes\n",
    "topics = [G.nodes[n].get(\"topic\") for n in G.nodes()]\n",
    "\n",
    "topic_counts = Counter(topics)\n",
    "\n",
    "for topic, count in sorted(topic_counts.items()):\n",
    "    print(topic, count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ebd590-b4b5-4e77-966e-ba5b0a625b53",
   "metadata": {},
   "source": [
    "We can see that clusters have somewhat different sizes, ranging from 75 to 220 nodes, showing that no extremely small or large clusters exist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff6d2fb-5fa4-48b1-b0c5-22c401b3e376",
   "metadata": {},
   "source": [
    "Now, we make use of a TF-IDF vectorizer, removing stopwords, to build a document-feature matrix out of the texts contained in our network (titles and descriptions of nodes as used above). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "095e6282-2a71-4b34-a67a-9aee3a5b707b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    stop_words=\"english\",\n",
    "    max_features=5000\n",
    ")\n",
    "\n",
    "tfidf_matrix = vectorizer.fit_transform(texts)\n",
    "terms = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97934f6c-c863-4a56-937d-e1e5011dad9b",
   "metadata": {},
   "source": [
    "Next, we define a function that retrieves the top ten terms for each topic/cluster found above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da666d79-5734-4f42-9a6f-aa515734d954",
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_terms_for_topic(topic_id, n=10):\n",
    "    # find all rows belonging to this topic\n",
    "    idx = [i for i, t in enumerate(cluster_labels) if t == topic_id]\n",
    "    \n",
    "    # average TF-IDF score for each term across documents in this topic\n",
    "    sub = tfidf_matrix[idx].mean(axis=0)\n",
    "    \n",
    "    # get top n word indices\n",
    "    top_indices = np.asarray(sub).ravel().argsort()[::-1][:n]\n",
    "    \n",
    "    return [terms[i] for i in top_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc695cf-faf3-407e-93e1-4fde08f61863",
   "metadata": {},
   "source": [
    "Following this, we get a set of unique labels or terms for each cluster, and print those for further inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e8bc172-2a92-4fd4-b94f-3f30e4a6609e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: island, islands, coast, km, mi, north, west, visit, south, largest\n",
      "Topic 1: city, population, county, largest, visit, capital, province, area, lake, europe\n",
      "Topic 2: zoo, park, theme, aquarium, disney, located, opened, world, resort, visit\n",
      "Topic 3: australia, sydney, south, melbourne, beach, park, australian, harbour, kilometres, tasmania\n",
      "Topic 4: state, states, united, national, visit, located, memorial, american, washington, historic\n",
      "Topic 5: museum, art, collection, visit, located, national, history, arts, gallery, united\n",
      "Topic 6: park, national, lake, canyon, state, river, visit, natural, colorado, wildlife\n",
      "Topic 7: city, visit, san, argentina, park, la, el, spanish, located, brazil\n",
      "Topic 8: island, islands, park, zealand, national, visit, new, reef, marine, waters\n",
      "Topic 9: city, history, vibrant, offers, rich, promises, unforgettable, bustling, visit, blend\n",
      "Topic 10: festival, held, attend, event, music, marathon, annual, day, world, year\n",
      "Topic 11: mountain, mount, highest, mountains, trail, peak, climb, ft, feet, metres\n",
      "Topic 12: japan, japanese, city, china, prefecture, tokyo, bay, kyoto, visit, temple\n",
      "Topic 13: read, novel, book, published, books, author, story, written, american, novels\n",
      "Topic 14: learn, used, use, form, called, practice, art, game, usually, light\n",
      "Topic 15: castle, england, visit, london, gardens, house, city, cathedral, palace, royal\n",
      "Topic 16: temple, ancient, city, site, visit, heritage, world, archaeological, unesco, century\n",
      "Topic 17: band, born, album, released, concert, american, songwriter, singer, guitar, vocals\n",
      "Topic 18: country, africa, republic, south, east, north, visit, asia, language, west\n",
      "Topic 19: make, dish, cheese, cake, cream, food, dough, baked, homemade, milk\n"
     ]
    }
   ],
   "source": [
    "unique_topics = sorted(set(cluster_labels))\n",
    "\n",
    "for topic in unique_topics:\n",
    "    words = top_terms_for_topic(topic, n=10)\n",
    "    print(f\"Topic {topic}: {', '.join(words)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57e70de-350a-49f7-9bb0-8fb71dc3717e",
   "metadata": {},
   "source": [
    "While there are some words overlapping between topics, most clusters seem to make sense and have a sound and coherent topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74a8403-ceb0-4f69-8214-a431b4d68953",
   "metadata": {},
   "source": [
    "In order to use the keywords assigned to each topic/cluster for inspiration in the open coding phase, we add them as node attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd87c82c-d0e8-49d5-b779-cada6df3b0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords = {\n",
    "    topic: top_terms_for_topic(topic, n=10)\n",
    "    for topic in unique_topics\n",
    "}\n",
    "\n",
    "for node in G.nodes():\n",
    "    t = G.nodes[node][\"topic\"]\n",
    "    G.nodes[node][\"topic_keywords_SBERT\"] = topic_keywords[t]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f744bc85-6ef5-4faa-b3e2-6b11815ea392",
   "metadata": {},
   "source": [
    "Finally, we export the network to excel in order to conduct a manual open coding process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1c1299b-7d1a-4769-8ce3-f6cd22724d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "for node, attrs in G.nodes(data=True):\n",
    "    row = {\"node_id\": node}\n",
    "    row.update(attrs)\n",
    "    data.append(row)\n",
    "\n",
    "# Create dataframe\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Export to Excel\n",
    "df.to_excel(\"../Data/Validation/graph_nodes.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
