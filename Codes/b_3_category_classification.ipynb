{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bf450a1",
   "metadata": {},
   "source": [
    "# B.3 Category Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a9959",
   "metadata": {},
   "source": [
    "This notebook contains our LLM pipeline for the category classification. It additionally incorporates our initial location classification (whether a specific location is mentioned or not), however, we decided to not utilize these results for our analysis, as we conducted an additional, more precise location classification in notebook B.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5330cac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Besitzer\\Desktop\\M.Sc. Social Data Science\\3. Semester\\Social graphs and interactions\\DayZero\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pickle\n",
    "import yaml\n",
    "import os\n",
    "import google.generativeai as genai\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f75c739f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/nicosrp/The-Architecture-of-Aspiration-A-Network-Perspective-on-Human-Goals/main/Networks/Prior%20Network%20Versions/b1_network.pkl\"\n",
    "response = requests.get(url)\n",
    "response.raise_for_status()\n",
    "\n",
    "G = pickle.loads(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce62cfd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['title', 'description', 'wants_to_do', 'have_done', 'comments', 'tags', 'included_by_our_users', 'merged_goals'])\n"
     ]
    }
   ],
   "source": [
    "# show all node attribute names\n",
    "print(next(iter(G.nodes(data=True)))[1].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbcaf398",
   "metadata": {},
   "source": [
    "## Setup Classification Pipeline\n",
    "\n",
    "Load configuration and set classification parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf5a272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 categories for classification\n",
      "Loaded 2 location categories for classification\n",
      "Using model: models/gemma-3-27b-it\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "with open('../Data/Classification Setup Data/config.yaml', 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load categories\n",
    "with open('../Data/Classification Setup Data/categories.yaml', 'r') as f:\n",
    "    categories_config = yaml.safe_load(f)\n",
    "    \n",
    "print(f\"Loaded {len(categories_config['categories'])} categories for classification\")\n",
    "print(f\"Loaded {len(categories_config['locations'])} location categories for classification\")\n",
    "print(f\"Using model: {config['api']['model']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e78fbc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key loaded successfully\n",
      "Classification Mode: MUTUALLY EXCLUSIVE (single category)\n",
      "Include Reasoning: Yes\n",
      "Nodes to process: All\n",
      "Starting from node: 0\n"
     ]
    }
   ],
   "source": [
    "# CLASSIFICATION SETTINGS\n",
    "\n",
    "# Load API key from config file\n",
    "GEMINI_API_KEY = config['api'].get('api_key', None)\n",
    "if not GEMINI_API_KEY:\n",
    "    # Try environment variable as fallback\n",
    "    GEMINI_API_KEY = os.getenv('GEMINI_API_KEY', None)\n",
    "    \n",
    "if not GEMINI_API_KEY:\n",
    "    print(\"WARNING: API_KEY not found!\")\n",
    "    print(\"Please set it in config.yaml or as environment variable\")\n",
    "else:\n",
    "    print(\"API key loaded successfully\")\n",
    "\n",
    "# Classification mode\n",
    "MUTUALLY_EXCLUSIVE = True\n",
    "# Justification settings\n",
    "INCLUDE_REASONING = True  # Set to False to skip reasoning\n",
    "\n",
    "# Processing options\n",
    "MAX_NODES_TO_PROCESS = None  # Set to a number (e.g., 100) to test on subset, None for all nodes\n",
    "START_FROM_NODE = 0  # Skip first N nodes\n",
    "SAVE_PROGRESS_EVERY = 50  # Save intermediate results every N nodes\n",
    "\n",
    "# Display settings\n",
    "VERBOSE = True  # Show detailed progress\n",
    "SHOW_SAMPLE_CLASSIFICATIONS = 5  # Number of sample results to display\n",
    "\n",
    "print(f\"Classification Mode: {'MUTUALLY EXCLUSIVE (single category)' if MUTUALLY_EXCLUSIVE else 'MULTI-LABEL (multiple categories)'}\")\n",
    "print(f\"Include Reasoning: {'Yes' if INCLUDE_REASONING else 'No'}\")\n",
    "print(f\"Nodes to process: {MAX_NODES_TO_PROCESS if MAX_NODES_TO_PROCESS else 'All'}\")\n",
    "print(f\"Starting from node: {START_FROM_NODE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3112d7d1",
   "metadata": {},
   "source": [
    "## Initialize Gemini API and Classification Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4916f6df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini model initialized successfully\n",
      "  Note: Using Gemma model - JSON extraction from text\n"
     ]
    }
   ],
   "source": [
    "# Configure Gemini API\n",
    "if GEMINI_API_KEY:\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    \n",
    "    # Initialize model with generation config\n",
    "    generation_config = {\n",
    "        \"temperature\": config['api']['temperature'],\n",
    "        \"top_p\": config['api']['top_p'],\n",
    "        \"max_output_tokens\": config['api']['max_tokens'],\n",
    "    }\n",
    "    \n",
    "    # Only add JSON mode for models that support it (not Gemma)\n",
    "    if 'gemma' not in config['api']['model'].lower():\n",
    "        generation_config[\"response_mime_type\"] = \"application/json\"\n",
    "    \n",
    "    model = genai.GenerativeModel(\n",
    "        model_name=config['api']['model'],\n",
    "        generation_config=generation_config,\n",
    "    )\n",
    "    \n",
    "    print(\"Gemini model initialized successfully\")\n",
    "    if 'gemma' in config['api']['model'].lower():\n",
    "        print(\"  Note: Using Gemma model - JSON extraction from text\")\n",
    "else:\n",
    "    print(\"Cannot initialize model without API key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ed06733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classification_prompt(title: str, description: str, categories: list, locations: list, mutually_exclusive: bool, include_reasoning: bool) -> str:\n",
    "    \"\"\"\n",
    "    Create a prompt for the LLM to classify a goal into both category and location.\n",
    "    \"\"\"\n",
    "    categories_text = \"\\n\".join([f\"- {cat['name']}: {cat['description']}\" for cat in categories])\n",
    "    locations_text = \"\\n\".join([f\"- {loc['name']}: {loc['description']}\" for loc in locations])\n",
    "    \n",
    "    if mutually_exclusive:\n",
    "        category_instruction = \"Select ONLY ONE category that best matches this goal.\"\n",
    "        if include_reasoning:\n",
    "            output_format = '''{\"category\": \"Category Name\", \"category_confidence\": 0.XX, \"location\": \"Location Category\", \"location_confidence\": 0.XX, \"reasoning\": \"Brief explanation\"}'''\n",
    "        else:\n",
    "            output_format = '''{\"category\": \"Category Name\", \"category_confidence\": 0.XX, \"location\": \"Location Category\", \"location_confidence\": 0.XX}'''\n",
    "    else:\n",
    "        category_instruction = \"Select categories that truly fit this goal. Be selective - only include categories where you have HIGH confidence (≥ 0.75). Multiple categories should only be selected if the goal genuinely belongs to each of them with strong confidence.\"\n",
    "        if include_reasoning:\n",
    "            output_format = '''{\"categories\": [\"Category 1\", \"Category 2\"], \"category_confidence\": [0.XX, 0.XX], \"location\": \"Location Category\", \"location_confidence\": 0.XX, \"reasoning\": \"Brief explanation\"}'''\n",
    "        else:\n",
    "            output_format = '''{\"categories\": [\"Category 1\", \"Category 2\"], \"category_confidence\": [0.XX, 0.XX], \"location\": \"Location Category\", \"location_confidence\": 0.XX}'''\n",
    "    \n",
    "    location_instruction = \"Additionally, classify the LOCATION MENTIONED in the goal into exactly ONE of the location categories below. Focus on any geographical references, places, or locations mentioned.\"\n",
    "    \n",
    "    prompt = f\"\"\"You are a goal classification expert. Classify the following goal into both a category and a location type.\n",
    "\n",
    "Goal Title: {title}\n",
    "Goal Description: {description}\n",
    "\n",
    "Focus primarily on the Goal Title for classification. Use the Goal Description only if the title is unclear or ambiguous.\n",
    "\n",
    "Available Categories:\n",
    "{categories_text}\n",
    "\n",
    "{category_instruction}\n",
    "\n",
    "Available Location Categories:\n",
    "{locations_text}\n",
    "\n",
    "{location_instruction}\n",
    "\n",
    "Return your response as JSON in this exact format:\n",
    "{output_format}\n",
    "\n",
    "Ensure:\n",
    "1. Category names match exactly from the categories list above\n",
    "2. Location category names match exactly from the locations list above\n",
    "3. All confidence values are between 0 and 1\n",
    "4. For location, select exactly ONE category that best describes the type of location mentioned (or \"No Location mentioned\" if none)\n",
    "\"\"\"\n",
    "    \n",
    "    return prompt\n",
    "\n",
    "\n",
    "def classify_goal(node_id: str, node_data: dict, model, categories: list, locations: list, mutually_exclusive: bool, include_reasoning: bool) -> dict:\n",
    "    \"\"\"\n",
    "    Classify a single goal using the LLM into both category and location.\n",
    "    \"\"\"\n",
    "    title = node_data.get('title', '')\n",
    "    description = node_data.get('description', '')\n",
    "    \n",
    "    # Handle missing data\n",
    "    if not title and not description:\n",
    "        return {\n",
    "            'node_id': node_id,\n",
    "            'node_attributes': node_data,\n",
    "            'classification': None,\n",
    "            'error': 'No title or description available'\n",
    "        }\n",
    "    \n",
    "    # Create prompt\n",
    "    prompt = create_classification_prompt(title, description, categories, locations, mutually_exclusive, include_reasoning)\n",
    "    \n",
    "    try:\n",
    "        # Call API\n",
    "        response = model.generate_content(prompt)\n",
    "        \n",
    "        # Parse JSON response\n",
    "        response_text = response.text.strip()\n",
    "        \n",
    "        # Try to extract JSON from response (handles both pure JSON and text with JSON)\n",
    "        try:\n",
    "            classification_data = json.loads(response_text)\n",
    "        except json.JSONDecodeError:\n",
    "            # If direct parsing fails, try to extract JSON from markdown code blocks\n",
    "            import re\n",
    "            json_match = re.search(r'```(?:json)?\\s*(\\{.*?\\})\\s*```', response_text, re.DOTALL)\n",
    "            if json_match:\n",
    "                classification_data = json.loads(json_match.group(1))\n",
    "            else:\n",
    "                # Try to find any JSON object in the text\n",
    "                json_match = re.search(r'\\{.*\\}', response_text, re.DOTALL)\n",
    "                if json_match:\n",
    "                    classification_data = json.loads(json_match.group(0))\n",
    "                else:\n",
    "                    raise ValueError(f\"Could not extract JSON from response: {response_text[:200]}\")\n",
    "        \n",
    "        return {\n",
    "            'node_id': node_id,\n",
    "            'node_attributes': node_data,\n",
    "            'classification': classification_data,\n",
    "            'error': None\n",
    "        }\n",
    "    \n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'node_id': node_id,\n",
    "            'node_attributes': node_data,\n",
    "            'classification': None,\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8b131e",
   "metadata": {},
   "source": [
    "## Test Classification\n",
    "\n",
    "Here, we run a quick test on a single node to verify everything works before processing all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f41a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing classification on a sample node...\n",
      "Node ID: dQggEQQH\n",
      "Title: Make ice cream from scratch\n",
      "Description: Ice cream or ice-cream is a frozen dessert usually made from dairy products, such as milk and cream, and often combined with fruits or other ingredients and flavours. Most varieties contain sugar, alt...\n",
      "Test Result:\n",
      "Classification: {'category': 'Food', 'category_confidence': 0.95, 'location': 'No location mentioned', 'location_confidence': 0.98, 'reasoning': 'The goal explicitly involves making food (ice cream). The goal does not mention any specific location.'}\n",
      "==================================================\n",
      "\n",
      "Test successful!\n",
      "Test Result:\n",
      "Classification: {'category': 'Food', 'category_confidence': 0.95, 'location': 'No location mentioned', 'location_confidence': 0.98, 'reasoning': 'The goal explicitly involves making food (ice cream). The goal does not mention any specific location.'}\n",
      "==================================================\n",
      "\n",
      "Test successful!\n"
     ]
    }
   ],
   "source": [
    "# Test on a single node\n",
    "test_node = list(G.nodes(data=True))[0]\n",
    "test_node_id, test_node_data = test_node\n",
    "\n",
    "print(\"Testing classification on a sample node...\")\n",
    "print(f\"Node ID: {test_node_id}\")\n",
    "print(f\"Title: {test_node_data.get('title', 'N/A')}\")\n",
    "print(f\"Description: {test_node_data.get('description', 'N/A')[:200]}...\")\n",
    "\n",
    "# Run classification\n",
    "test_result = classify_goal(\n",
    "    node_id=test_node_id,\n",
    "    node_data=test_node_data,\n",
    "    model=model,\n",
    "    categories=categories_config['categories'],\n",
    "    locations=categories_config['locations'],\n",
    "    mutually_exclusive=MUTUALLY_EXCLUSIVE,\n",
    "    include_reasoning=INCLUDE_REASONING\n",
    ")\n",
    "\n",
    "print(\"Test Result:\")\n",
    "if test_result['error']:\n",
    "    print(f\"Error: {test_result['error']}\")\n",
    "else:\n",
    "    print(f\"Classification: {test_result['classification']}\")\n",
    "print(f\"{'='*50}\\n\")\n",
    "print(\"Test successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae8662",
   "metadata": {},
   "source": [
    "## Run Classification Pipeline\n",
    "\n",
    "This cell will process all nodes and classify them using the LLM. We did not run the classificatio cell again after getting the results form our first run, therefore there is no output displayed. However, we created a subsequent cell where we call our previous results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45151ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize results storage\n",
    "classification_results = []\n",
    "\n",
    "# Get all nodes\n",
    "all_nodes = list(G.nodes(data=True))\n",
    "print(f\"Total nodes in graph: {len(all_nodes)}\")\n",
    "\n",
    "# Determine which nodes to process\n",
    "if MAX_NODES_TO_PROCESS:\n",
    "    nodes_to_process = all_nodes[START_FROM_NODE:START_FROM_NODE + MAX_NODES_TO_PROCESS]\n",
    "else:\n",
    "    nodes_to_process = all_nodes[START_FROM_NODE:]\n",
    "\n",
    "print(f\"Processing {len(nodes_to_process)} nodes (starting from node {START_FROM_NODE})\")\n",
    "\n",
    "# Process each node\n",
    "print(\"\\nStarting classification...\\n\")\n",
    "request_delay = config['rate_limit']['delay_between_requests']\n",
    "\n",
    "for idx, (node_id, node_data) in enumerate(tqdm(nodes_to_process, desc=\"Classifying goals\")):\n",
    "    \n",
    "    # Classify the goal\n",
    "    result = classify_goal(\n",
    "        node_id=node_id,\n",
    "        node_data=node_data,\n",
    "        model=model,\n",
    "        categories=categories_config['categories'],\n",
    "        locations=categories_config['locations'],\n",
    "        mutually_exclusive=MUTUALLY_EXCLUSIVE,\n",
    "        include_reasoning=INCLUDE_REASONING\n",
    "    )\n",
    "    \n",
    "    classification_results.append(result)\n",
    "    \n",
    "    # Show sample results\n",
    "    if VERBOSE and idx < SHOW_SAMPLE_CLASSIFICATIONS:\n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(f\"  Node ID: {result['node_id']}\")\n",
    "        print(f\"  Title: {result['node_attributes'].get('title', '')[:60]}...\")\n",
    "        if result['error']:\n",
    "            print(f\"  Error: {result['error']}\")\n",
    "        else:\n",
    "            print(f\"  Classification: {result['classification']}\")\n",
    "    \n",
    "    # Save progress periodically\n",
    "    if SAVE_PROGRESS_EVERY and (idx + 1) % SAVE_PROGRESS_EVERY == 0:\n",
    "        with open('../Data/Classification Results/classification1_results.json', 'w') as f:\n",
    "            json.dump(classification_results, f, indent=2)\n",
    "        if VERBOSE:\n",
    "            print(f\"\\nProgress saved at {idx + 1} nodes\")\n",
    "    \n",
    "    # Rate limiting\n",
    "    time.sleep(request_delay)\n",
    "\n",
    "print(f\"\\nTotal processed: {len(classification_results)}\")\n",
    "print(f\"Errors: {sum(1 for r in classification_results if r['error'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d39527b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded classification_results from ../Data/Classification Results/classification1_results.json (2890 entries)\n",
      "Total classification_results entries available: 2890\n"
     ]
    }
   ],
   "source": [
    "# Re-load previous classification results if a JSON file exists in this folder\n",
    "json_fname = '../Data/Classification Results/classification1_results.json'\n",
    "request_delay = config['rate_limit']['delay_between_requests']\n",
    "if os.path.exists(json_fname):\n",
    "    try:\n",
    "        with open(json_fname, 'r', encoding='utf-8') as f:\n",
    "            classification_results = json.load(f)\n",
    "        print(f'Loaded classification_results from {json_fname} ({len(classification_results)} entries)')\n",
    "    except Exception as e:\n",
    "        print(f'Error loading {json_fname}: {e}')\n",
    "        # fallback: keep existing in-memory variable if present\n",
    "        if 'classification_results' not in globals():\n",
    "            classification_results = []\n",
    "else:\n",
    "    if 'classification_results' in globals():\n",
    "        print(f'No {json_fname} found. Using in-memory classification_results ({len(classification_results)} entries).')\n",
    "    else:\n",
    "        print(f'No {json_fname} found and no in-memory classification_results; creating empty list.')\n",
    "        classification_results = []\n",
    "\n",
    "# Ensure the rest of the notebook can run from here (counts, etc.)\n",
    "print(f'Total classification_results entries available: {len(classification_results)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be77694",
   "metadata": {},
   "source": [
    "## Retry Failed Classifications\n",
    "\n",
    "This cell will retry classification for any goals that failed during the initial pipeline run. We run this as long as we get no errors anymore. Notably, the only remaining error we get is re-occuring due to \"PROHIBITED_CONTENT\". This goal describes reading the book \"Lolita\", which is controversial due to its plot where an adult is lusting after a minor, leading to this error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5597b749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 failed classifications to retry\n",
      "Retrying failed classifications...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying failed classifications:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still failed for node Csz8rzLw: Invalid operation: The `response.parts` quick accessor requires a single candidate, but but `response.candidates` is empty.\n",
      "This appears to be caused by a blocked prompt, see `response.prompt_feedback`: block_reason: PROHIBITED_CONTENT\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying failed classifications: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Retry complete!\n",
      "Remaining errors: 1\n"
     ]
    }
   ],
   "source": [
    "# Retry failed classifications\n",
    "failed_indices = [i for i, result in enumerate(classification_results) if result['error']]\n",
    "print(f\"Found {len(failed_indices)} failed classifications to retry\")\n",
    "\n",
    "if failed_indices:\n",
    "    print(\"Retrying failed classifications...\")\n",
    "    \n",
    "    for idx in tqdm(failed_indices, desc=\"Retrying failed classifications\"):\n",
    "        result = classification_results[idx]\n",
    "        node_id = result['node_id']\n",
    "        node_data = result['node_attributes']\n",
    "        \n",
    "        # Retry classification\n",
    "        retry_result = classify_goal(\n",
    "            node_id=node_id,\n",
    "            node_data=node_data,\n",
    "            model=model,\n",
    "            categories=categories_config['categories'],\n",
    "            locations=categories_config['locations'],\n",
    "            mutually_exclusive=MUTUALLY_EXCLUSIVE,\n",
    "            include_reasoning=INCLUDE_REASONING\n",
    "        )\n",
    "        \n",
    "        # Update the result if retry was successful\n",
    "        if not retry_result['error']:\n",
    "            classification_results[idx] = retry_result\n",
    "            print(f\"Fixed classification for node {node_id}\")\n",
    "        else:\n",
    "            print(f\"Still failed for node {node_id}: {retry_result['error']}\")\n",
    "        \n",
    "        # Rate limiting\n",
    "        time.sleep(request_delay)\n",
    "    \n",
    "    # Save updated results\n",
    "    with open('../Data/Classification Results/classification1_results.json', 'w') as f:\n",
    "        json.dump(classification_results, f, indent=2)\n",
    "    \n",
    "    final_errors = sum(1 for r in classification_results if r['error'])\n",
    "    print(f\"\\nRetry complete!\")\n",
    "    print(f\"Remaining errors: {final_errors}\")\n",
    "else:\n",
    "    print(\"No failed classifications to retry.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e69a0a2",
   "metadata": {},
   "source": [
    "## Inspect Unique Classification Entries\n",
    "\n",
    "Display all unique category and location entries to identify any incorrect formulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ce00a9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "UNIQUE CLASSIFICATION ENTRIES\n",
      "==================================================\n",
      "\n",
      "Unique Categories (15):\n",
      "  - Academic and professional achievements\n",
      "  - Acts of kindness/altruism\n",
      "  - Creativity\n",
      "  - Events & concerts\n",
      "  - Experiences\n",
      "  - Food\n",
      "  - Health\n",
      "  - Media consumption\n",
      "  - Nature\n",
      "  - New skills\n",
      "  - Places of interest\n",
      "  - Relationships & social life\n",
      "  - Religion\n",
      "  - Sports\n",
      "  - Travel destinations\n",
      "\n",
      "Unique Locations (2):\n",
      "  - Location mentioned\n",
      "  - No location mentioned\n",
      "\n",
      "==================================================\n",
      "All classifications use expected category and location names.\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Collect all unique categories and locations from successful classifications\n",
    "unique_categories = set()\n",
    "unique_locations = set()\n",
    "\n",
    "for result in classification_results:\n",
    "    if result['classification'] and not result['error']:\n",
    "        if MUTUALLY_EXCLUSIVE:\n",
    "            # Single category mode\n",
    "            category = result['classification'].get('category', '')\n",
    "            location = result['classification'].get('location', '')\n",
    "            if category:\n",
    "                unique_categories.add(category)\n",
    "            if location:\n",
    "                unique_locations.add(location)\n",
    "        else:\n",
    "            # Multi-label mode\n",
    "            categories = result['classification'].get('categories', [])\n",
    "            location = result['classification'].get('location', '')\n",
    "            for cat in categories:\n",
    "                if cat:\n",
    "                    unique_categories.add(cat)\n",
    "            if location:\n",
    "                unique_locations.add(location)\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(\"UNIQUE CLASSIFICATION ENTRIES\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"\\nUnique Categories ({len(unique_categories)}):\")\n",
    "for cat in sorted(unique_categories):\n",
    "    print(f\"  - {cat}\")\n",
    "\n",
    "print(f\"\\nUnique Locations ({len(unique_locations)}):\")\n",
    "for loc in sorted(unique_locations):\n",
    "    print(f\"  - {loc}\")\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "\n",
    "# Check for potential issues\n",
    "expected_categories = {cat['name'] for cat in categories_config['categories']}\n",
    "expected_locations = {loc['name'] for loc in categories_config['locations']}\n",
    "\n",
    "unexpected_categories = unique_categories - expected_categories\n",
    "unexpected_locations = unique_locations - expected_locations\n",
    "\n",
    "if unexpected_categories:\n",
    "    print(f\"WARNING: Found {len(unexpected_categories)} unexpected category entries:\")\n",
    "    for cat in sorted(unexpected_categories):\n",
    "        print(f\"    - {cat}\")\n",
    "\n",
    "if unexpected_locations:\n",
    "    print(f\"WARNING: Found {len(unexpected_locations)} unexpected location entries:\")\n",
    "    for loc in sorted(unexpected_locations):\n",
    "        print(f\"    - {loc}\")\n",
    "\n",
    "if not unexpected_categories and not unexpected_locations:\n",
    "    print(\"All classifications use expected category and location names.\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072781fa",
   "metadata": {},
   "source": [
    "Create a structured Excel file with all classification results for manual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52f882fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Excel file saved: ../Data/Classification Results/classification2_results.xlsx\n",
      "\n",
      "DataFrame shape: (2890, 9)\n",
      "\n",
      "First few rows:\n",
      "    node_id                                              title  \\\n",
      "0  dQggEQQH                        Make ice cream from scratch   \n",
      "1  nnEnjr7O  Leave an inspirational note inside a book for ...   \n",
      "2  nusyxQLs                                         Fly a kite   \n",
      "3  MDTcjx4X                                       Go skydiving   \n",
      "4  GM7VTx2U                      Visit the Library of Congress   \n",
      "\n",
      "                                         description error  \\\n",
      "0  Ice cream or ice-cream is a frozen dessert usu...  None   \n",
      "1  Imagine the joy of discovering a heartfelt mes...  None   \n",
      "2  A kite is a tethered heavier-than-air or light...  None   \n",
      "3  Parachuting, including also skydiving, is a me...  None   \n",
      "4  The Library of Congress is the research librar...  None   \n",
      "\n",
      "                    category category_confidence               location  \\\n",
      "0                       Food                0.95  No location mentioned   \n",
      "1  Acts of kindness/altruism                0.95  No location mentioned   \n",
      "2                Experiences                0.95  No location mentioned   \n",
      "3                Experiences                0.95  No location mentioned   \n",
      "4         Places of interest                0.95     Location mentioned   \n",
      "\n",
      "  location_confidence                                          reasoning  \n",
      "0                0.98  The goal explicitly involves making food (ice ...  \n",
      "1                0.98  The goal centers around performing a kind deed...  \n",
      "2                0.98  Flying a kite is an activity done for enjoymen...  \n",
      "3                0.98  Skydiving is an adventurous activity and falls...  \n",
      "4                0.98  The goal explicitly states visiting a famous l...  \n",
      "\n",
      "==================================================\n",
      "Classification Summary:\n",
      "==================================================\n",
      "\n",
      "Category distribution:\n",
      "Travel destinations                       1009\n",
      "Places of interest                         866\n",
      "Nature                                     291\n",
      "Events & concerts                          256\n",
      "New skills                                 108\n",
      "Food                                        96\n",
      "Experiences                                 83\n",
      "Media consumption                           80\n",
      "Sports                                      46\n",
      "Health                                      17\n",
      "Religion                                    17\n",
      "Relationships & social life                  8\n",
      "Creativity                                   6\n",
      "Acts of kindness/altruism                    4\n",
      "Academic and professional achievements       2\n",
      "                                             1\n",
      "Name: category, dtype: int64\n",
      "\n",
      "Mean category confidence: 0.949\n",
      "\n",
      "Location distribution:\n",
      "Location mentioned       2591\n",
      "No location mentioned     298\n",
      "                            1\n",
      "Name: location, dtype: int64\n",
      "\n",
      "Mean location confidence: 0.971\n",
      "\n",
      "Errors: 1\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for Excel export\n",
    "excel_data = []\n",
    "\n",
    "for result in classification_results:\n",
    "    row = {\n",
    "        'node_id': result['node_id'],\n",
    "        'title': result['node_attributes'].get('title', ''),\n",
    "        'description': result['node_attributes'].get('description', ''),\n",
    "        'error': result['error']\n",
    "    }\n",
    "    \n",
    "    if result['classification'] and not result['error']:\n",
    "        if MUTUALLY_EXCLUSIVE:\n",
    "            # Single category case\n",
    "            row['category'] = result['classification'].get('category', '')\n",
    "            row['category_confidence'] = result['classification'].get('category_confidence', '')\n",
    "            row['location'] = result['classification'].get('location', '')\n",
    "            row['location_confidence'] = result['classification'].get('location_confidence', '')\n",
    "            row['reasoning'] = result['classification'].get('reasoning', '')\n",
    "        else:\n",
    "            # Multiple categories case\n",
    "            categories_list = result['classification'].get('categories', [])\n",
    "            confidences_list = result['classification'].get('category_confidence', [])\n",
    "            row['categories'] = ', '.join(categories_list) if categories_list else ''\n",
    "            row['category_confidences'] = ', '.join([str(c) for c in confidences_list]) if confidences_list else ''\n",
    "            row['location'] = result['classification'].get('location', '')\n",
    "            row['location_confidence'] = result['classification'].get('location_confidence', '')\n",
    "            row['reasoning'] = result['classification'].get('reasoning', '')\n",
    "    else:\n",
    "        if MUTUALLY_EXCLUSIVE:\n",
    "            row['category'] = ''\n",
    "            row['category_confidence'] = ''\n",
    "            row['location'] = ''\n",
    "            row['location_confidence'] = ''\n",
    "            row['reasoning'] = ''\n",
    "        else:\n",
    "            row['categories'] = ''\n",
    "            row['category_confidences'] = ''\n",
    "            row['location'] = ''\n",
    "            row['location_confidence'] = ''\n",
    "            row['reasoning'] = ''\n",
    "    \n",
    "    excel_data.append(row)\n",
    "\n",
    "# Create DataFrame\n",
    "df_results = pd.DataFrame(excel_data)\n",
    "\n",
    "# Save to Excel with formatting\n",
    "output_filename = f'../Data/Classification Results/classification2_results.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_filename, engine='openpyxl') as writer:\n",
    "    df_results.to_excel(writer, sheet_name='Classifications', index=False)\n",
    "    \n",
    "    # Get the worksheet\n",
    "    worksheet = writer.sheets['Classifications']\n",
    "    \n",
    "    # Adjust column widths\n",
    "    for column in worksheet.columns:\n",
    "        max_length = 0\n",
    "        column_letter = column[0].column_letter\n",
    "        for cell in column:\n",
    "            try:\n",
    "                if len(str(cell.value)) > max_length:\n",
    "                    max_length = len(str(cell.value))\n",
    "            except:\n",
    "                pass\n",
    "        adjusted_width = min(max_length + 2, 50)  # Cap at 50 characters\n",
    "        worksheet.column_dimensions[column_letter].width = adjusted_width\n",
    "\n",
    "print(f\"Excel file saved: {output_filename}\")\n",
    "print(f\"\\nDataFrame shape: {df_results.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(df_results.head())\n",
    "\n",
    "# Display summary statistics\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Classification Summary:\")\n",
    "print(f\"{'='*50}\")\n",
    "\n",
    "if MUTUALLY_EXCLUSIVE:\n",
    "    category_counts = df_results['category'].value_counts()\n",
    "    print(f\"\\nCategory distribution:\")\n",
    "    print(category_counts)\n",
    "    \n",
    "    # Calculate mean confidence, handling empty strings\n",
    "    category_confidences = pd.to_numeric(df_results['category_confidence'], errors='coerce').dropna()\n",
    "    if not category_confidences.empty:\n",
    "        print(f\"\\nMean category confidence: {category_confidences.mean():.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nMean category confidence: N/A (no valid confidence values)\")\n",
    "    \n",
    "    location_counts = df_results['location'].value_counts()\n",
    "    print(f\"\\nLocation distribution:\")\n",
    "    print(location_counts)\n",
    "    \n",
    "    # Calculate mean location confidence, handling empty strings\n",
    "    location_confidences = pd.to_numeric(df_results['location_confidence'], errors='coerce').dropna()\n",
    "    if not location_confidences.empty:\n",
    "        print(f\"\\nMean location confidence: {location_confidences.mean():.3f}\")\n",
    "    else:\n",
    "        print(f\"\\nMean location confidence: N/A (no valid confidence values)\")\n",
    "else:\n",
    "    # For multi-label, count how many goals have each category\n",
    "    all_categories = []\n",
    "    for cats in df_results['categories']:\n",
    "        if cats:\n",
    "            all_categories.extend([c.strip() for c in str(cats).split(',')])\n",
    "    category_counts = pd.Series(all_categories).value_counts()\n",
    "    print(f\"\\nCategory distribution (multi-label):\")\n",
    "    print(category_counts)\n",
    "    \n",
    "    location_counts = df_results['location'].value_counts()\n",
    "    print(f\"\\nLocation distribution:\")\n",
    "    print(location_counts)\n",
    "\n",
    "errors_count = df_results['error'].notna().sum()\n",
    "print(f\"\\nErrors: {errors_count}\")\n",
    "print(f\"{'='*50}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c551b7",
   "metadata": {},
   "source": [
    "## Assign Classifications to Network Nodes\n",
    "\n",
    "This will add the classification data as new node attributes in the network graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "869e2c08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Node Attribute Assignment Complete\n",
      "==================================================\n",
      "Nodes updated: 2889\n",
      "Nodes failed: 0\n",
      "Total nodes in graph: 2890\n",
      "\n",
      "==================================================\n",
      "Sample Node with Classification:\n",
      "==================================================\n",
      "Node ID: dQggEQQH\n",
      "Title: Make ice cream from scratch\n",
      "Category: Food\n",
      "Category Confidence: 0.95\n",
      "Location: No location mentioned\n",
      "Location Confidence: 0.98\n",
      "Reasoning: The goal explicitly involves making food (ice cream). The goal does not mention any specific locatio...\n"
     ]
    }
   ],
   "source": [
    "# Add classification attributes to nodes\n",
    "nodes_updated = 0\n",
    "nodes_failed = 0\n",
    "\n",
    "for result in classification_results:\n",
    "    node_id = result['node_id']\n",
    "    \n",
    "    # Check if node exists in graph\n",
    "    if node_id not in G.nodes():\n",
    "        nodes_failed += 1\n",
    "        continue\n",
    "    \n",
    "    # Add classification data to node\n",
    "    if result['error']:\n",
    "        G.nodes[node_id]['classification_error'] = result['error']\n",
    "        G.nodes[node_id]['classified'] = False\n",
    "    elif result['classification']:\n",
    "        if MUTUALLY_EXCLUSIVE:\n",
    "            # Single category\n",
    "            G.nodes[node_id]['category'] = result['classification'].get('category', '')\n",
    "            G.nodes[node_id]['category_confidence'] = result['classification'].get('category_confidence', 0)\n",
    "            G.nodes[node_id]['location'] = result['classification'].get('location', '')\n",
    "            G.nodes[node_id]['location_confidence'] = result['classification'].get('location_confidence', 0)\n",
    "            G.nodes[node_id]['classification_reasoning'] = result['classification'].get('reasoning', '')\n",
    "            G.nodes[node_id]['classified'] = True\n",
    "        else:\n",
    "            # Multiple categories - store as list\n",
    "            G.nodes[node_id]['categories'] = result['classification'].get('categories', [])\n",
    "            G.nodes[node_id]['category_confidences'] = result['classification'].get('category_confidence', [])\n",
    "            G.nodes[node_id]['location'] = result['classification'].get('location', '')\n",
    "            G.nodes[node_id]['location_confidence'] = result['classification'].get('location_confidence', 0)\n",
    "            G.nodes[node_id]['classification_reasoning'] = result['classification'].get('reasoning', '')\n",
    "            G.nodes[node_id]['classified'] = True\n",
    "        \n",
    "        nodes_updated += 1\n",
    "    else:\n",
    "        G.nodes[node_id]['classified'] = False\n",
    "        nodes_failed += 1\n",
    "\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Node Attribute Assignment Complete\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Nodes updated: {nodes_updated}\")\n",
    "print(f\"Nodes failed: {nodes_failed}\")\n",
    "print(f\"Total nodes in graph: {G.number_of_nodes()}\")\n",
    "\n",
    "# Show sample node with new attributes\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Sample Node with Classification:\")\n",
    "print(f\"{'='*50}\")\n",
    "for node_id, node_data in list(G.nodes(data=True))[:1]:\n",
    "    if node_data.get('classified'):\n",
    "        print(f\"Node ID: {node_id}\")\n",
    "        print(f\"Title: {node_data.get('title', 'N/A')}\")\n",
    "        if MUTUALLY_EXCLUSIVE:\n",
    "            print(f\"Category: {node_data.get('category', 'N/A')}\")\n",
    "            print(f\"Category Confidence: {node_data.get('category_confidence', 'N/A')}\")\n",
    "            print(f\"Location: {node_data.get('location', 'N/A')}\")\n",
    "            print(f\"Location Confidence: {node_data.get('location_confidence', 'N/A')}\")\n",
    "        else:\n",
    "            print(f\"Categories: {node_data.get('categories', 'N/A')}\")\n",
    "            print(f\"Category Confidences: {node_data.get('category_confidences', 'N/A')}\")\n",
    "            print(f\"Location: {node_data.get('location', 'N/A')}\")\n",
    "            print(f\"Location Confidence: {node_data.get('location_confidence', 'N/A')}\")\n",
    "        print(f\"Reasoning: {node_data.get('classification_reasoning', 'N/A')[:100]}...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e8eec0",
   "metadata": {},
   "source": [
    "## Final Save: Export Updated Network to Pickle\n",
    "\n",
    "Save the network with all classifications to a pickle file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4f7bf16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All node attribute names after classification:\n",
      "dict_keys(['title', 'description', 'wants_to_do', 'have_done', 'comments', 'tags', 'included_by_our_users', 'merged_goals', 'category', 'category_confidence', 'location', 'location_confidence', 'classification_reasoning', 'classified'])\n"
     ]
    }
   ],
   "source": [
    "# show all node attribute names after update\n",
    "print(f\"\\nAll node attribute names after classification:\")\n",
    "print(next(iter(G.nodes(data=True)))[1].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "352ab785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final classified network saved to: ../Networks/Prior Network Versions/b3_network.pkl\n",
      "Network contains 2890 nodes and 219130 edges\n",
      "All classification data has been preserved in the pickle file.\n"
     ]
    }
   ],
   "source": [
    "# Save the final updated network with classifications to pickle\n",
    "output_network_file = f'../Networks/Prior Network Versions/b3_network.pkl'\n",
    "\n",
    "with open(output_network_file, 'wb') as f:\n",
    "    pickle.dump(G, f)\n",
    "\n",
    "print(f\"Final classified network saved to: {output_network_file}\")\n",
    "print(f\"Network contains {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "print(f\"All classification data has been preserved in the pickle file.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
