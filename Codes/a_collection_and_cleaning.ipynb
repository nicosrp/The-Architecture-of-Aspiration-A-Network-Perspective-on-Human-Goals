{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6530f85f-cc1f-4b61-ab6b-f786ae05d2b4",
   "metadata": {},
   "source": [
    "# A - Data Collection & Scraping - Scraping the DayZero Project Platform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977ba4a1-87f1-4318-8a37-ee8f5af08491",
   "metadata": {},
   "source": [
    "In this notebook we scrape all necessary data from the website DayZero Project and start cleaning the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce36e051-8590-4132-9f12-e351afb515ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant packages\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import json\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c23e93c-3623-4701-89e3-949ceed77a04",
   "metadata": {},
   "source": [
    "To create a network of goals, we used data from the DayZero Project platform, a website and community of goal setters, in existing since 2009. Our aim was to create a network of goals, connected by co-occurence on users' lists. Due to the structure of the DayZero Project platform, the scraping process was relatively complicated. The website does not offer any overview of all users (or goals). The only way to access users' list is by going to their page using their username. The only way to retrieve users' usernames in a semi-systematic manner is to use the search function, which returns a limited number (at most 50) of usernames per search query. \n",
    "\n",
    "The process of extracting the necessary data went as follows:\n",
    "1. We started with a list of the 400 most common names from the 2000s in the US. We retrieved this list from the social security website of the US government. Due to scraping restrictions, we had to manually save the html file of the page. We decided to use this as basis for extracting usernames from the website.\n",
    "2. We used each of the 400 names in a search query, scraping the html of the page of search results and extracting all usernames from the html. This left us with 12998 unique usernames (15205 originally, as some search queries lead to duplicates).\n",
    "3. We then looped over the 12998 user names to retrieve the list of goals for each, scraping the html of their lists and then extracting goals. Each goal has a unique ID which we extracted. This gave us a list of 231269 unique goals (503828 goals before clearing out duplicates).\n",
    "4. Using the goals' IDs, which form the unique part of a goal URL, we scraped the html from all goal pages and extracted titles and descriptions for each goal.\n",
    "5. Next, we extracted further attributes of each goal, such as tags, comments, and counts of completion and inclusion on lists (these attributes do not refer to the user cohort we extracted from the website, but are counts provided by the website relating to all users).\n",
    "6. Finally, we compiled dictionaries for further processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080ec649-8c01-4cb7-bb54-c245d51d7c2f",
   "metadata": {},
   "source": [
    "## 1. Importing File With Most Common Names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8752a0-4f16-4321-83e3-140a6ab29028",
   "metadata": {},
   "source": [
    "The first step in the the process of gathering our data is to read in the html file containing the 400 names used a base as described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7def97d5-8832-43a0-937e-69396d18ec2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Male Names: ['Jacob', 'Michael', 'Joshua', 'Matthew', 'Daniel', 'Christopher', 'Andrew', 'Ethan', 'Joseph', 'William']\n",
      "Female Names: ['Emily', 'Madison', 'Emma', 'Olivia', 'Hannah', 'Abigail', 'Isabella', 'Samantha', 'Elizabeth', 'Ashley']\n"
     ]
    }
   ],
   "source": [
    "# Load the local HTML file\n",
    "tables = pd.read_html(\"../Data/Additional Data/2000_names_USA.html\")\n",
    "\n",
    "# Usually the first table is the one we want\n",
    "names_df = tables[0]\n",
    "\n",
    "# Rename columns for clarity\n",
    "names_df.columns = [\"Rank\", \"Male Name\", \"Male Count\", \"Female Name\", \"Female Count\"]\n",
    "\n",
    "# Extract male and female names\n",
    "male_names = names_df[\"Male Name\"].tolist()\n",
    "female_names = names_df[\"Female Name\"].tolist()\n",
    "\n",
    "print(\"Male Names:\", male_names[:10])\n",
    "print(\"Female Names:\", female_names[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd741120-9ddd-4aca-bbf7-714b82c0755e",
   "metadata": {},
   "outputs": [],
   "source": [
    "male_names = male_names[:-1]\n",
    "female_names = female_names[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "251c903c-a511-45b7-9eab-453a397eb89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_list = female_names + male_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4efa5f-c0bb-4fc7-ad74-5a5f7ef55538",
   "metadata": {},
   "source": [
    "The names_list contains the 400 usernames to be used in the search in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013c120f-f453-4321-a9d0-4acb9e74d29e",
   "metadata": {},
   "source": [
    "## 2. Scraping Usernames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b354e1cf-52a3-458c-8e34-a18e41bc04ae",
   "metadata": {},
   "source": [
    "### Creating a Cookie File to Simulate Logging In"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef66951-2509-405e-a433-4b3e179b8d43",
   "metadata": {},
   "source": [
    "In order to use the websites search function, one needs to be logged in as a user. Therefore, we are saving the cookies from a manual log in to use in the scraping process further on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425bd3d1-b89a-448e-a2d8-cfd8861e7153",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://dayzeroproject.com\")\n",
    "\n",
    "# pause here so you can log in manually\n",
    "input(\"Log in in the browser, then press Enter here...\")\n",
    "\n",
    "# after login, save cookies\n",
    "cookies = driver.get_cookies()\n",
    "cookie_file = Path(\"cookies.json\")\n",
    "cookie_file.write_text(json.dumps(cookies, indent=2))\n",
    "\n",
    "driver.quit()\n",
    "print(\"Saved cookies to cookies.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4462d403-2a31-4ae3-b967-fcbd058d1757",
   "metadata": {},
   "source": [
    "### Retrieving Usernames Based on Search Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b9d5f5-5183-4a9e-86a8-da23386d5011",
   "metadata": {},
   "source": [
    "The first step in retrieving usernames based on search pages is to perform a search for each of the 400 names from the names_list above and scrape the html of each search results page. We save this into a list called search_pages_html_list. For safekeeping, we export the list as a json file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b4d87a-ea19-4f62-9a62-3bb775acb195",
   "metadata": {},
   "source": [
    "#### Scraping Search Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2f15c4-b450-4db4-bf3d-fe91a01cc9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# 1. Setup Selenium\n",
    "# -----------------------------\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# -----------------------------\n",
    "# 2. Go to the site first\n",
    "# -----------------------------\n",
    "driver.get(\"https://dayzeroproject.com\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Add cookies for login\n",
    "# -----------------------------\n",
    "\n",
    "# path to your saved cookie file\n",
    "cookie_file = Path(\"cookies.json\")  # adjust path if needed\n",
    "\n",
    "# load cookies from JSON file\n",
    "with cookie_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    cookies = json.load(f)\n",
    "\n",
    "# add cookies to the Selenium driver\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "\n",
    "# Refresh to apply cookies\n",
    "driver.refresh()\n",
    "time.sleep(2)\n",
    "\n",
    "# -----------------------------\n",
    "# 3. Go to the user's following page\n",
    "# -----------------------------\n",
    "USERNAME = \"Rebecca2025\"\n",
    "driver.get(f\"https://dayzeroproject.com/user/{USERNAME}/following\")\n",
    "time.sleep(2)\n",
    "\n",
    "# -----------------------------\n",
    "# 4. Click the FIND PEOPLE button (JS click)\n",
    "# -----------------------------\n",
    "find_people_button = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.CSS_SELECTOR, \"span.showfindpeople.hand\"))\n",
    ")\n",
    "driver.execute_script(\"arguments[0].click();\", find_people_button)\n",
    "\n",
    "# -----------------------------\n",
    "# 5. Wait for the search box div to appear\n",
    "# -----------------------------\n",
    "search_container = WebDriverWait(driver, 20).until(\n",
    "    EC.presence_of_element_located((By.ID, \"following-searchbox\"))\n",
    ")\n",
    "search_input = search_container.find_element(By.TAG_NAME, \"input\")\n",
    "search_button = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_element_located((By.ID, \"following-search\"))\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 6. Loop over names_list\n",
    "# -----------------------------\n",
    "search_pages_html_list = []  # store HTML for each search\n",
    "\n",
    "for name in names_list:\n",
    "    # clear previous input and type new name\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(name)\n",
    "\n",
    "    # click the SEARCH button\n",
    "    driver.execute_script(\"arguments[0].click();\", search_button)\n",
    "\n",
    "    # wait a few seconds for JS to populate results\n",
    "    time.sleep(5)  # adjust if results load slowly\n",
    "\n",
    "    # optionally click again if needed (keep your original logic)\n",
    "    driver.find_element(By.ID, \"following-search\").click()\n",
    "    time.sleep(3)\n",
    "\n",
    "    # get full page HTML and store\n",
    "    page_html = driver.page_source\n",
    "    search_pages_html_list.append({\"name\": name, \"html\": page_html})\n",
    "    print(f\"Saved HTML for search: {name}\")\n",
    "\n",
    "print(\"All search pages saved. You can now parse results with regex or BeautifulSoup.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8574c49a-90c0-4d67-8be0-b6ad07dbbfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------------\n",
    "# save the list of HTML pages\n",
    "# -----------------------------\n",
    "output_file = Path(\"dayzero_search_pages.json\")\n",
    "\n",
    "with output_file.open(\"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(search_pages_html_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(search_pages_html_list)} pages to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0cd527-c15e-4271-822b-6ab64d55cd7d",
   "metadata": {},
   "source": [
    "Using the html pages of the search queries, we extract the usernames by using beautiful soup and regex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48db673f-6569-47b1-bbfb-ee2061b23cda",
   "metadata": {},
   "source": [
    "#### Extracting Usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "807b6a5d-40c9-498d-b65a-b3714b91100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames = []  # this will hold usernames from all pages\n",
    "\n",
    "for page in search_pages_html_list:\n",
    "    html = page[\"html\"]\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # find all divs with class 'following-username'\n",
    "    user_divs = soup.find_all(\"div\", class_=\"following-username\")\n",
    "\n",
    "    for div in user_divs:\n",
    "        a_tag = div.find(\"a\", class_=\"bold\")\n",
    "        if a_tag and \"href\" in a_tag.attrs:\n",
    "            href = a_tag[\"href\"]\n",
    "            username = href.split(\"/user/\")[-1]\n",
    "            all_usernames.append(username)\n",
    "\n",
    "print(f\"Extracted {len(all_usernames)} usernames in total.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f8e980-2e53-4fcc-adda-4684275bd826",
   "metadata": {},
   "outputs": [],
   "source": [
    "username_counts = Counter(all_usernames)\n",
    "duplicates = {u: c for u, c in username_counts.items() if c > 1}\n",
    "\n",
    "print(f\"Total usernames: {len(all_usernames)}\")\n",
    "print(f\"Unique usernames: {len(username_counts)}\")\n",
    "print(f\"Number of duplicates: {len(duplicates)}\")\n",
    "\n",
    "# Optional: see top repeated usernames\n",
    "for u, c in sorted(duplicates.items(), key=lambda x: -x[1])[:20]:\n",
    "    print(u, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70592e00-d725-40a7-84de-fcc2ab7dc9dc",
   "metadata": {},
   "source": [
    "This leaves us with 12998 unique usernames."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d095d5-3eb2-4e9d-8f8d-a057bcba4a56",
   "metadata": {},
   "source": [
    "## 3. Extracting Goals of Users"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cde40e-23b4-44f8-87f8-42d970bdcbda",
   "metadata": {},
   "source": [
    "Using the list of usernames, we now scrape users' lists of goals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e316ca77-e992-4a1c-8eab-2547ca252b28",
   "metadata": {},
   "source": [
    "### Scraping Users' Goal Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4649ac70-c958-487d-bc1e-1c6c04df71f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_lists_html_list = []\n",
    "\n",
    "# --- 1. Start a new driver session ---\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "# or headless mode if you don’t need the UI:\n",
    "# options.add_argument(\"--headless\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options)\n",
    "\n",
    "# --- 2. Load cookies if the site requires login ---\n",
    "import json\n",
    "with open(\"cookies.json\", \"r\") as f:\n",
    "    cookies = json.load(f)\n",
    "\n",
    "driver.get(\"https://dayzeroproject.com\")  # open the domain first\n",
    "for cookie in cookies:\n",
    "    driver.add_cookie(cookie)\n",
    "\n",
    "# loop\n",
    "\n",
    "for user in tqdm(all_usernames, desc=\"Fetching goal pages\"):\n",
    "    try:\n",
    "        url = f\"https://dayzeroproject.com/user/{user}/list/-1\"\n",
    "        driver.get(url)\n",
    "\n",
    "        # wait until the goals are visible or the page stabilizes\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"div#content\"))\n",
    "        )\n",
    "        time.sleep(2)  # short delay for JS completion\n",
    "\n",
    "        html = driver.page_source\n",
    "        goal_lists_html_list.append({\"username\": user, \"html\": html})\n",
    "    except Exception as e:\n",
    "        print(f\"Error for {user}: {e}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eacd6fe-69aa-476f-840b-2b3a135a9e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the path and filename\n",
    "output_file = \"goal_lists_html_list.json\"\n",
    "\n",
    "# save the list of dictionaries to JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(goal_lists_html_list, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Saved {len(goal_lists_html_list)} pages to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7734f54-600c-4399-93b3-1b4a0d2efe91",
   "metadata": {},
   "source": [
    "The goal_lists_html_list now holds the html of all goals we scraped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ce637e-62c1-48df-878c-3a72954b460d",
   "metadata": {},
   "source": [
    "Now we can extract a list of goals (a dict here to include necessary information) by using beautiful soup and regex again."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c421b2a4-a9ea-4578-b1c9-04e723dd2485",
   "metadata": {},
   "source": [
    "### Extracting Goals From Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5cb8f2e-e95f-4e59-b39e-e8b6f7408643",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_goals = {}\n",
    "\n",
    "for item in tqdm(goal_lists_html_list, desc=\"Extracting user goals\"):\n",
    "    username = item.get(\"username\")\n",
    "    html = item.get(\"html\")\n",
    "\n",
    "    # Skip invalid entries\n",
    "    if not username or not isinstance(html, str):\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # -----------------------------\n",
    "    # Extract all goals\n",
    "    # -----------------------------\n",
    "    goals = []\n",
    "    seen_ids = set()\n",
    "\n",
    "    for a_tag in soup.find_all(\"a\", href=True):\n",
    "        href = a_tag[\"href\"]\n",
    "        if href.startswith(\"/goal/\"):\n",
    "            goal_id = href.split(\"/goal/\")[-1]\n",
    "            if goal_id not in seen_ids:\n",
    "                text = a_tag.get_text(strip=True)\n",
    "                goals.append({\"id\": goal_id, \"href\": href, \"text\": text})\n",
    "                seen_ids.add(goal_id)\n",
    "\n",
    "    user_goals[username] = goals\n",
    "\n",
    "print(f\"Extracted goals for {len(user_goals)} users.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc4b5a94-b252-4950-8517-32b07b86e8ea",
   "metadata": {},
   "source": [
    "The result of this is a dictionary called user_goals with user name as key and goals of the user as values (nested dict with ID, href and text of goal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8f0b2-ae61-4798-bcbe-f9f2b8402aff",
   "metadata": {},
   "source": [
    "Not all users have any goals on their list, so we split the dictionary into two parts: one containing users with goals, and one users without."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939079e7-babc-44ec-a86d-d851ef53d698",
   "metadata": {},
   "source": [
    "### Filtering Users With and Without Goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2934971e-27d9-411c-9c6c-3ab728c1e188",
   "metadata": {},
   "outputs": [],
   "source": [
    "users_with_goals = {}\n",
    "users_without_goals = {}\n",
    "\n",
    "for username, goals in tqdm(user_goals.items(), desc=\"Filtering users\", unit=\"user\"):\n",
    "    if goals:  # non-empty list is truthy\n",
    "        users_with_goals[username] = goals\n",
    "    else:\n",
    "        users_without_goals[username] = goals\n",
    "\n",
    "print(f\"Users with goals: {len(users_with_goals)}\")\n",
    "print(f\"Users without goals: {len(users_without_goals)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdce0340-b3f9-4a24-bb6f-b6f4ec411fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save user_goals to JSON\n",
    "with open(\"users_with_goals.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(users_with_goals, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"users_with_goals has been saved to 'users_with_goals.json'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d6e462-32be-4614-b1f3-a8bea5426077",
   "metadata": {},
   "source": [
    "This results in 10087 with goals and 2910 without."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3938fe0-6f03-4003-8bd7-fc8b0a98ece3",
   "metadata": {},
   "source": [
    "### Checking Up on Total Goals Extracted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b95239-1c64-4095-a762-e5910f3c0083",
   "metadata": {},
   "source": [
    "Since the dictionary of the users with goals has a list with nested dictionaries (one per goal) as value, we can now already see how many goals we have scraped, namely 503828, which may include duplicate goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6caee81-ca59-4e58-9abb-fe4d1a3484d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_goals = sum(len(goals) for goals in users_with_goals.values())\n",
    "print(f\"Total goals (including duplicates): {total_goals}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a56e0b-d3cc-4103-b4b7-232e3674a5c9",
   "metadata": {},
   "source": [
    "## Creating a Dictionairy of Unique Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824ab56f-d4aa-42e1-8132-eef247fb8da7",
   "metadata": {},
   "source": [
    "Since the dictionary above contains duplicates, we filter it to create a new dictionary called goals_dict, containing only unique goals. This gives us 231269 goals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed150981-9e0a-4845-b8ba-b1c0858028ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the goal container\n",
    "goals_dict = {}\n",
    "\n",
    "for username, goals in tqdm(users_with_goals.items(), desc=\"Processing users\"):\n",
    "    for goal in goals:\n",
    "        goal_id = goal[\"id\"]\n",
    "        title = goal[\"text\"]\n",
    "        \n",
    "        # Only add if not already in the dict (to avoid duplicates)\n",
    "        if goal_id not in goals_dict:\n",
    "            goals_dict[goal_id] = {\n",
    "                \"title\": title,\n",
    "                \"html\": None  # placeholder for HTML to be fetched later\n",
    "            }\n",
    "\n",
    "print(f\"Total unique goals collected: {len(goals_dict)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6260db73-16a9-4fad-ac8e-34e4d751e382",
   "metadata": {},
   "source": [
    "## 4. Retrieving Goals from the Goal Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311bb233-be7c-4db9-a6f6-a4f562f415c5",
   "metadata": {},
   "source": [
    "Now that we have the IDs of all the unique goals we have scraped, we can loop over them to extract the html of each goal page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f86f69-f5f3-4c4a-9eb5-370217b64d67",
   "metadata": {},
   "source": [
    "### Scraping Goal Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde3c390-9845-4498-9ce5-447b5d0110da",
   "metadata": {},
   "source": [
    "We start by defining a function to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf06331-5a32-47aa-ad23-5aeb76ddc954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch a single goal page\n",
    "def fetch_goal_html(goal_id, max_retries=3, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Fetches the HTML for a goal with a retry mechanism.\n",
    "    Returns (goal_id, html, error_message or None)\n",
    "    \"\"\"\n",
    "    url = f\"https://dayzeroproject.com/goal/{goal_id}\"\n",
    "    \n",
    "    for attempt in range(1, max_retries + 1):\n",
    "        try:\n",
    "            response = requests.get(url, timeout=15)\n",
    "            response.raise_for_status()\n",
    "            return goal_id, response.text, None  # success\n",
    "        \n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            if attempt < max_retries:\n",
    "                sleep_time = backoff_factor ** (attempt - 1)\n",
    "                print(f\"⚠️ Attempt {attempt} failed for {goal_id}: {error_msg}. Retrying in {sleep_time}s...\")\n",
    "                time.sleep(sleep_time)\n",
    "            else:\n",
    "                print(f\"❌ Failed for {goal_id} after {max_retries} attempts.\")\n",
    "                return goal_id, None, error_msg\n",
    "\n",
    "# Function to save progress periodically\n",
    "def save_goals_dict(filename=\"goals_dict_progress.json\"):\n",
    "    with open(filename, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(goals_dict, f, ensure_ascii=False)\n",
    "    print(f\"Progress saved to {filename}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce4496-9d94-4e0f-8d83-f25ba265857e",
   "metadata": {},
   "source": [
    "Then we apply this function in our loop to retrieve all html pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cea781-d05c-44a7-b59a-eb6387ada0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "all_goal_ids = list(goals_dict.keys())\n",
    "batch_size = 1000  # number of goals per batch\n",
    "max_workers = 10   # adjust based on network & server tolerance\n",
    "\n",
    "for i in range(0, len(all_goal_ids), batch_size):\n",
    "    batch_goal_ids = all_goal_ids[i:i+batch_size]\n",
    "    print(f\"Processing batch {i//batch_size + 1} ({len(batch_goal_ids)} goals)...\")\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(fetch_goal_html, gid): gid for gid in batch_goal_ids}\n",
    "        \n",
    "        for future in tqdm(as_completed(futures), total=len(futures), desc=f\"Batch {i//batch_size + 1}\"):\n",
    "            goal_id, html, error = future.result()\n",
    "            goals_dict[goal_id][\"html\"] = html\n",
    "            if error:\n",
    "                print(f\"❌ Error fetching {goal_id}: {error}\")\n",
    "\n",
    "    # Polite pause between batches\n",
    "    time.sleep(5)\n",
    "    \n",
    "    # Save progress after each batch\n",
    "    save_goals_dict()\n",
    "\n",
    "print(\"Finished fetching HTML for all goals.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f0a2db-b3e4-49e8-88c0-595f496b5d1d",
   "metadata": {},
   "source": [
    "One important aspect of goals is that some have descriptions, while others do not. For our analysis, we will only use goals which have a description in order to be able to fully analyze them. To separate those goals with text from those without, we first need to extract the text from the html."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3868247-15d9-48f0-b873-bd2f7ab71fea",
   "metadata": {},
   "source": [
    "### Extracting Text from Goal Pages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30af5ca-ad0c-4aab-a7eb-51198f06975d",
   "metadata": {},
   "source": [
    "First, we use beautiful soup and regex to extract the description from the html. For this, we first retrieve certain divs, and then the text within those divs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db7d8d5-af36-4356-acad-8b8ff568b8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_description_dict = {}\n",
    "no_description_id_list = []\n",
    "\n",
    "for goal_id, goal_data in tqdm(goals_dict.items(), desc=\"Extracting goal texts\"):\n",
    "    html = goal_data.get(\"html\", \"\")\n",
    "    \n",
    "    if not html:\n",
    "        # No HTML, empty list and track ID\n",
    "        goal_description_dict[goal_id] = []\n",
    "        no_description_id_list.append(goal_id)\n",
    "        continue\n",
    "\n",
    "    soup = BeautifulSoup(html, \"html.parser\")  # or \"lxml\" if installed\n",
    "    goal_texts = []\n",
    "\n",
    "    # Find all outer divs\n",
    "    outer_divs = soup.find_all(\"div\", class_=\"ml10 mr10\")\n",
    "\n",
    "    for outer in outer_divs:\n",
    "        # Find all nested darkdarkdarkgrey divs\n",
    "        inner_divs = outer.find_all(\"div\", class_=\"darkdarkdarkgrey\")\n",
    "        for div in inner_divs:\n",
    "            # Remove nested attribution divs\n",
    "            for attrib in div.find_all(\"div\", class_=\"goal-attribution\"):\n",
    "                attrib.decompose()\n",
    "            \n",
    "            text = div.get_text(strip=True)\n",
    "            if text:\n",
    "                goal_texts.append(text)\n",
    "\n",
    "    # Save in final dict; track IDs with no descriptions\n",
    "    if goal_texts:\n",
    "        goal_description_dict[goal_id] = goal_texts\n",
    "    else:\n",
    "        goal_description_dict[goal_id] = []\n",
    "        no_description_id_list.append(goal_id)\n",
    "\n",
    "print(f\"Total goals processed: {len(goal_description_dict)}\")\n",
    "print(f\"Goals with no description: {len(no_description_id_list)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3483696a-c9b4-4b7d-9fd7-d4bdc6a34116",
   "metadata": {},
   "source": [
    "Our approach of retrieving divs leads to some goals having more than one description, if the description is parted into several paragraphs in the html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6f170cd-3332-4e6a-90a7-dcc1febbf26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter goals with more than 1 description\n",
    "goals_multiple_texts = {gid: texts \n",
    "                        for gid, texts in goal_description_dict.items() \n",
    "                        if len(texts) > 1}\n",
    "\n",
    "print(f\"Found {len(goals_multiple_texts)} goals with more than 1 description.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbccf95-b05f-4bde-aec7-605f9e58d422",
   "metadata": {},
   "source": [
    "Therefore, we merge the text if there are several items in the description list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1e5da-1d48-49a4-b26e-52c4e611245c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for goal_id, texts in goal_description_dict.items():\n",
    "    if len(texts) > 1:\n",
    "        # Merge all items into one string, separated by space (or newline if preferred)\n",
    "        merged_text = \" \".join(texts)\n",
    "        goal_description_dict[goal_id] = [merged_text]  # keep as a single-item list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89cbd6-ab1a-49ea-87c5-22ea7bc9a737",
   "metadata": {},
   "source": [
    "As every goal should now have a description list of length 1, we convert it to a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bae825e-432b-4822-af8d-5a4044a5f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for goal_id, texts in goal_description_dict.items():\n",
    "    if texts:  # should always be True since each list has 1 item\n",
    "        goal_description_dict[goal_id] = texts[0]  # convert list to string\n",
    "    else:\n",
    "        goal_description_dict[goal_id] = \"\"  # just in case some lists are empty"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea6645a-0a5f-4e80-8684-2fec84244f1d",
   "metadata": {},
   "source": [
    "Finally, we created an updated dictionary with descriptions and export this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5e8d18-7468-48c7-8607-fef3c240c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "goal_dict_with_description_updated = {}\n",
    "\n",
    "for goal_id, description in goal_description_dict.items():\n",
    "    title = goal_dict[goal_id].get(\"title\", \"\")\n",
    "    goal_dict_with_description_updated[goal_id] = {\n",
    "        \"title\": title,\n",
    "        \"description\": description\n",
    "    }\n",
    "\n",
    "print(f\"Created updated goal dict with {len(goal_dict_with_description_updated)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e239f3-e60a-48d4-9f26-cdd856b8b573",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"goal_dict_with_description_updated.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(goal_dict_with_description_updated, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported {len(goal_dict_with_description_updated)} goals to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc05d972-ba0a-412b-b279-42c49fbea183",
   "metadata": {},
   "source": [
    "## 5. Extracting Further Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a8a4d8-9f9c-467c-bc54-4e6dd4db4ee9",
   "metadata": {},
   "source": [
    "There are three additional attributes we are interested in extracting.\n",
    "1. Counts of how many people have completed a certain goal and how many people want to complete it. We extract these in two step process, first getting a messy div class with the correct content, and later cleaning the content in a second step.\n",
    "2. The comments on a goal. Not all goals have comments, and many comments are pictures, which we are not extracting. However, if there are textual comments, we are extracting them for each goal where this is applicable.\n",
    "3. The tags assigned by the website itself to each goal. A goal can have one or many tags, which we therefore extract as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320768b0-a6d7-47c7-b334-30a7ab28b696",
   "metadata": {},
   "outputs": [],
   "source": [
    "for goal_id, content in tqdm(goals_dict.items(), desc=\"Parsing goals\"):\n",
    "    html = content.get(\"html\", \"\")\n",
    "    try:\n",
    "        doc = lxml.html.fromstring(html)\n",
    "\n",
    "        # 1. Counts\n",
    "        node = doc.xpath('//div[@class=\"places-peoplecount size90\"]')\n",
    "        counts_messy_dict[goal_id] = node[0].text_content().strip() if node else None\n",
    "\n",
    "        # 2. Comments\n",
    "        comments = doc.xpath('//div[@class=\"goal-notecontent rounded truncate\"]')\n",
    "        goals_comments_dict[goal_id] = [c.text_content().strip() for c in comments]\n",
    "\n",
    "        # 3. Tags\n",
    "        tags = doc.xpath('//a[starts-with(@href, \"/tag/\")]')\n",
    "        goals_tags_dict[goal_id] = [t.text_content().strip() for t in tags]\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error at ID:\", goal_id)\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb370fd-2463-40ed-8299-bda380b34b22",
   "metadata": {},
   "source": [
    "To correctly retrieve the counts and the different variations of text used to describe them, we need to clean them further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2742a251-efa9-4b03-9ff4-a90953530c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_clean_dict = {}\n",
    "\n",
    "for goal_id, text in tqdm(counts_messy_dict.items(), desc=\"Parsing counts\"):\n",
    "    if text is None:\n",
    "        counts_clean_dict[goal_id] = {\"wants_to_do\": 0, \"have_done\": 0}\n",
    "        continue\n",
    "\n",
    "    wants_to_do = 0\n",
    "    have_done = 0\n",
    "\n",
    "    # Remove HTML tags if present\n",
    "    text_clean = BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "    # Extract \"On X lists\"\n",
    "    match_lists = re.search(r\"On ([\\d,]+) lists\", text_clean)\n",
    "    if match_lists:\n",
    "        wants_to_do = int(match_lists.group(1).replace(\",\", \"\"))\n",
    "\n",
    "    # Extract \"X people want to do this\"\n",
    "    match_wants = re.search(r\"(\\d+|One) person(?:s)? want(?:s)? to do this\", text_clean)\n",
    "    if match_wants:\n",
    "        if match_wants.group(1) == \"One\":\n",
    "            wants_to_do = 1\n",
    "        else:\n",
    "            wants_to_do = int(match_wants.group(1).replace(\",\", \"\"))\n",
    "\n",
    "    # Extract \"X people have done it\"\n",
    "    match_done = re.search(r\"([\\d,]+) people have done it\", text_clean)\n",
    "    if match_done:\n",
    "        have_done = int(match_done.group(1).replace(\",\", \"\"))\n",
    "\n",
    "    counts_clean_dict[goal_id] = {\"wants_to_do\": wants_to_do, \"have_done\": have_done}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527ed99f-d634-4b90-a9e5-9e55c22235c3",
   "metadata": {},
   "source": [
    "Now we can export all three dictionaries for safekeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54f5bc-636b-4558-beae-6fc44255ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export counts_clean_dict\n",
    "with open(\"counts_clean_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(counts_clean_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Export goals_comments_dict\n",
    "with open(\"goals_comments_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(goals_comments_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Export goals_tags_dict\n",
    "with open(\"goals_tags_dict.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(goals_tags_dict, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ab45f-b376-418e-bc20-e74b008f6559",
   "metadata": {},
   "source": [
    "## 6. Compiling a Final Dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc33764-ddf5-40c8-aaea-16d6fc1f333f",
   "metadata": {},
   "source": [
    "Finally, now that we have retrieved all goals based on the usernames, and extracted the relevant attributes, we can combine all of this into one dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda47e62-ac13-48ee-af5f-64867c0ad4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "goals_with_attributes_dict = {}\n",
    "\n",
    "for goal_id, desc_info in goal_dict_with_description_updated.items():\n",
    "    # Get counts, or default to 0 for both keys if missing\n",
    "    counts = counts_clean_dict.get(goal_id, {\"wants_to_do\": 0, \"have_done\": 0})\n",
    "\n",
    "    combined_info = {\n",
    "        \"title\": desc_info.get(\"title\", \"\"),\n",
    "        \"description\": desc_info.get(\"description\", \"\"),\n",
    "        \"wants_to_do\": counts.get(\"wants_to_do\", 0),\n",
    "        \"have_done\": counts.get(\"have_done\", 0),\n",
    "        \"comments\": goals_comments_dict.get(goal_id, []),\n",
    "        \"tags\": goals_tags_dict.get(goal_id, [])\n",
    "    }\n",
    "    goals_with_attributes_dict[goal_id] = combined_info\n",
    "\n",
    "print(f\"Combined dict created with {len(goals_with_attributes_updated_dict)} items.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105840c-6326-46a8-8f14-a3a98f268b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"goals_with_attributes.json\"\n",
    "\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(goals_with_attributes_dict, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"Exported {len(goals_with_attributes_dict)} goals to {output_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406baa7a-b736-4431-9024-d782883b4d0c",
   "metadata": {},
   "source": [
    "In the further processing, this dictionary, as well as the users_with_goals dictionary will be used to conduct further pre-processing and construct our network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
